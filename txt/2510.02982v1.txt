[preprint,11pt,authoryear]elsarticle

[linesnumbered,ruled,vlined]algorithm2e

[percent]overpic

[1]#1

[ox]D. Dehtyriov 

[cor1]Corresponding author
[nd]J. F. MacArt

[ox]J. Sirignano

[ox]organization=Mathematical Institute, University of Oxford,
 addressline=Andrew Wiles Building, Woodstock Rd, 
 city=Oxford,
 postcode=OX2 6GG, 
 country=United Kingdom

[nd]organization=Department of Aerospace and Mechanical Engineering, University of Notre Dame,
 addressline=369 Fitzpatrick Hall of Engineering, 
 city=Notre Dame,
 postcode=IN 46556,
 country=USA

Deep learning (DL) has demonstrated promise for accelerating and enhancing the accuracy of flow physics simulations, but progress is constrained by the scarcity of high-fidelity training data, which is costly to generate and inherently limited to a small set of flow conditions. Consequently, closures trained in the conventional offline paradigm tend to overfit and fail to generalise to new regimes.
We introduce an online optimisation framework for DL-based Reynolds-averaged Navier--Stokes (RANS) closures which seeks to address the challenge of limited high-fidelity datasets. Training data is dynamically generated by embedding a direct numerical simulation (DNS) within a subdomain of the RANS domain. The RANS solution supplies boundary conditions to the DNS, while the DNS provides mean velocity and turbulence statistics that are used to update a DL closure model during the simulation. This feedback loop enables the closure to adapt to the embedded DNS target flow, avoiding reliance on precomputed datasets and improving out-of-distribution performance.
The approach is demonstrated for the stochastically forced Burgers equation and for turbulent channel flow at $Re_=180$, 270, 395 and 590 with varying embedded domain lengths $1 L_0/L 8$. Online-optimised RANS models significantly outperform both offline-trained and literature-calibrated closures, with accurate training achieved using modest DNS subdomains. Performance degrades primarily when boundary-condition contamination dominates or when domains are too short to capture low-wavenumber modes. This framework provides a scalable route to physics-informed machine learning closures, enabling data-adaptive reduced-order models that generalise across flow regimes without requiring large precomputed training datasets.

Fluid mechanics Turbulence modelling RANS Machine Learning

Introduction

Fluid turbulence in the continuum flow regime is fully described by the Navier--Stokes equations. Solving these equations exactly, i.e., direct numerical simulation (DNS), in flow regimes of engineering interest is typically infeasible due to the large range of spatiotemporal scales required to accurately resolve the nonlinear physics. To remain computationally feasible, simulations often reduce the necessary spatiotemporal resolution via large eddy simulation (LES) using spatial filtering or Reynolds-averaged Navier--Stokes (RANS) simulations using spatiotemporal averaging.
Both require physical approximations to be introduced via turbulence closure models.

High-fidelity simulation data has historically played a pivotal role in calibrating turbulence closure models. Many classical LES/RANS model parameters (e.g., the Kolmogorov constants, Smagorinsky coefficient or damping functions for near-wall behaviour) have been informed by matching DNS or experimental benchmarks . The availability of high-fidelity data has thus opened the door to data-driven turbulence modelling, wherein one uses measurements or simulations of the flow field to guide the form or parameters of the closure. A fundamental challenge, especially for deep learning closure models with large numbers of parameters, is the limited number of high-fidelity datasets which are typically available for calibrating closure models. 

Motivation

Areas such as computer vision and natural language processing have seen rapid progress in machine learning, in large part due to the availability of vast, high-quality datasets. In contrast, scientific applications, such as turbulence modelling, lack such data abundance. Turbulence data is typically generated via experiments or high-fidelity direct numerical simulation, both of which are expensive and limited to a finite set of fixed geometries or Reynolds numbers. DNS is computationally prohibitive at high Reynolds numbers, while experimental campaigns are constrained by cost, facility availability, and difficulties in measuring three-dimensional, time-resolved fields. As a result, real-world engineering applications suffer from data sparsity.

In a typical offline supervised learning workflow, this limited data is used to train the parameters $$ of a turbulence model, which is then deployed to new regimes without further adaptation. This often results in reduced accuracy, as the model must extrapolate beyond its training distribution. This traditional workflow proceeds as follows:

 Generate DNS data for a finite set of conditions

 t &= (v^; \ ), \ x ,

where \( \) represents the Navier--Stokes operator (or other nonlinear operator), $$ collects the conditions (Reynolds number, geometry, boundary conditions, forcing, etc.), and $^d$ is the $d$-dimensional Cartesian domain. Additional constraints such as the incompressibility condition
$0 = v^$
can also be imposed.

 Train a closure model $h_ij$ incorporated in the RANS/LES partial differential equation (PDE),

 t &= ( ;\ ) - x_j( ; \ ), \ x ,

where the overbar $$ represents a spatiotemporal average/filtering operation, and $h_ij$ is the learned closure parametrised by $$. Additional constraints can likewise be imposed,

0 &= .

Because $$ variables are filtered/averaged quantities, $v^$ must be filtered consistently before comparison. The model is trained by minimizing the discrepancy with the dataset:

 L() = _t T_|| - ||^2 \ dx \ dt,

where $T$ denotes the training time window.
 Make out-of-sample predictions for unseen conditions not included in $$.

Despite recent advances in deep learning for turbulence modelling, this offline paradigm remains fundamentally limited by the scope of the available high-fidelity training data, which is costly to generate and inherently restricted to a finite set of flow regimes. As a result, the trained model lacks the capacity to adapt when applied to flow conditions that lie outside the support of the training distribution, often leading to degraded predictions and a failure to generalise in physically meaningful ways.

To address this limitation, we propose to train turbulence closures using an embedded, online learning approach, where the closure model is trained online during the flow simulation itself. A high-fidelity DNS is embedded within a subdomain of a larger RANS simulation. The RANS solution provides boundary conditions to the embedded DNS, which in turn supplies the high-fidelity flow statistics needed to update the turbulence model parameters.

This feedback loop enables the model to adapt during the simulation, reducing the reliance on offline data and improving predictive accuracy across both domains. A schematic of the setup is shown in Figure~. The embedded, online learning workflow proceeds as follows:

 Simulate coupled high- and low-fidelity PDEs on two domains, where $_$ denotes the embedded, high-fidelity DNS (eDNS) subdomain, and $_$ denotes the surrounding low-fidelity RANS domain, with interface
$=__$:

 t &= \!(u^), 
\ \ x _, \\
0 &= u^, \\
0 &= 
\!(_(t)^) 
+ h\!( _(t)^;\,(t)),
\ \ x _, \\
0 &= ^_(t), \\
u^ |_ &= \!(_(t)^|_;\,(t)), 
\ \ x . 

Here $u=(u_1, , u_d)$ denotes the $d$-dimensional velocity vector (where $d = 1, 2,$ or $3$), and 
$h( \ ;)$ is a learned closure model producing a vector field whose 
divergence modifies the RANS momentum equation. We define the composite field $u$ by
$u=u^$ on $_$ and
$u=^$ on $_$, where $_^d$, $_^p$, and $p d$ depending on the number of statistically homogeneous dimensions.
The operator $$ supplies boundary data to the DNS subdomain from the surrounding RANS solution at the interface $$.
In the simplest case, $$ is the identity, directly imposing the RANS field on $$. In this work, $$ is time dependent and augments the RANS state with rescaled fluctuations to provide statistically consistent turbulent inflow (see section~ for details). Note that $u^$ depends on $$ only through $$ on $$; the interior operator $$ does not depend explicitly on $$.

 Continuously update the model parameters and boundary conditions for asymptotic minimisation of the closure-modelling error:

dt
= __
(u^ - _(t)^)\,
_\,_(t)^\, dx,
 (0)=_0,

where $$ is the learning rate, and where $ v_^$ denotes the RANS surrogate field solved on the entire domain $$. This is necessary because $^$ is only defined on $_$, while the parameter update in is evaluated over $_$. When the solution has statistically homogeneous dimensions, as in the channel flow test case considered herein, one may equivalently use $^$ in place of $^$ for computational efficiency. We adopt this convention throughout the remainder of the paper.

By comparison to the offline supervised learning approach, our framework generates training data directly from the exact physical conditions and geometries on which predictions are desired. This enables model training in computationally challenging flow regimes without the limitations of dataset sparsity and overfitting, since the reduced-order model is trained locally on the embedded DNS data but is then applied to the surrounding RANS domain to represent the unresolved dynamics, thereby generalizing to the remainder of the flow field outside the high-fidelity region. The present formulation can be readily extended to arbitrary closures such as the $k$--$$ and $k$--$$ models. More broadly, the strategy applies to any nonlinear PDE system where simplification (e.g., temporal averaging or spatial filtering) introduces unclosed terms.

Data driven turbulence modelling

The Reynolds-averaged Navier--Stokes equations, which solve only for the mean flow variables, remain in widespread use due to their low computational cost . This comes at the cost of an unclosed term with significant complexity, representing the effect of the fluctuating field on the mean flow. The time averaging shifts the challenge from the large computational effort required for solving the instantaneous equations to modelling the flow physics embedded in the unclosed RANS equations.

Conventional RANS models solve additional transport equations for the unclosed term, which themselves depend on closure coefficients. These coefficients are typically calibrated using data from canonical flows and based on asymptotic arguments , allowing for tractable `plug-and-play' solutions for arbitrary engineering flows of interest. Despite the widespread use of RANS however, it is well known that its predictive accuracy is poor in flows with strong anisotropy, separation or curvature, where the underlying assumptions break down . As RANS closures are tuned to a narrow set of canonical flows, they lack universal accuracy across a broad spectrum of turbulent flow configurations. This lack of universality has motivated extensive research into improved closure strategies, both through physics-based reasoning and, more recently, data-driven approaches.

A seminal contribution in this direction was the Tensor Basis Neural Network (TBNN) proposed by , which imposed Galilean invariance through a custom multiplicative layer to learn nonlinear mappings from local flow features to the anisotropy tensor. introduced an alternative strategy that learned the discrepancy between RANS-predicted and DNS-derived Reynolds stresses, enabling data-informed corrections to classical models. developed the Field Inversion and Machine Learning (FIML) framework, in which a spatially distributed modification to a RANS closure is inferred via inverse modelling and then generalised through supervised learning. Such approaches demonstrated that augmenting eddy-viscosity models with data-driven corrections can significantly improve RANS predictions for flows similar to the calibration cases.

Both DNS and well-resolved large-eddy simulation (LES) have provided detailed turbulence statistics that were historically unattainable from experiments alone. For instance, the DNS of fully developed channel flow resolved all essential scales of near-wall turbulence and reported a comprehensive set of statistics for comparison with experiments. These high-fidelity datasets may inform the physics of traditional RANS models, and they serve as a ground truth for developing and calibrating new models. 

However, limited data diversity leads to overfitting of the learned model to the calibration flows, yielding poor generalisation to new regimes . Offline-trained ML models often encode strong priors based on the training set and exhibit degraded performance when applied to flows with different geometries, Reynolds numbers, or dominant physics. This generalisation gap has spurred efforts to regularise models using physics-informed features, invariant bases, or sparsity-promoting architectures. Recent reviews have emphasised the importance of embedding physical constraints into ML turbulence models to ensure robustness and extrapolative power. Probabilistic learning approaches have also been introduced to provide uncertainty quantification (UQ) in ML-predicted closures. For instance, the Reynolds stress prediction can be formulated as a probabilistic mapping, allowing confidence intervals to be estimated alongside mean predictions .

More recent data-driven approaches have sought to directly embed machine-learned closures into the RANS or LES equations and optimise them against high-fidelity data. In these PDE-constrained formulations, the functional form of the unresolved terms is represented by a flexible model (such as a neural network), and its parameters are adjusted by requiring the RANS/LES solution to match reference data. Adjoint methods are typically employed to compute the gradient of the loss with respect to the closure parameters. provide a rigorous convergence analysis of this approach for a model elliptic PDE system, with adjoint-based optimisation then used to train a neural network functioning as a RANS closure model, calibrating it on several DNS datasets of turbulent channel flow. Similarly, developed a deep-learning LES subgrid closure by directly matching filtered DNS data for flow around various bluff bodies. , , and developed reinforcement learning methods to train wall models for LES. These examples underscore the potential of offline-trained deep-learning closures: when provided with sufficient high-fidelity data of a given flow class, the ML-based models can encode complex turbulent transport physics and improve upon conventional closures.

In parallel to these ML-driven strategies, non-ML approaches have also been developed to reduce the cost of incorporating high-fidelity information into RANS and LES closures. One class of methods is embedded DNS frameworks , where local fine-mesh DNS blocks are coupled to a global coarse-mesh domain through block-spectral mappings and source terms, reducing mesh-count scaling with Reynolds number compared to conventional LES or DNS. A complementary line of work focuses on boundary condition generation, where synthetic inflow turbulence methods generate realistic inflow statistics and correlations, reducing the domain length required to achieve fully developed turbulence. Both approaches illustrate how embedding or inflow strategies can lower the computational burden of integrating high-fidelity information into turbulence simulations.

Despite these advances, a central limitation of ML-based closures remains their reliance on offline training with precomputed high-fidelity datasets. Such models are constrained by dataset availability, limited flow diversity, and the attendant risk of overfitting. Preliminary work has explored online optimisation of LES closures using embedded DNS .

Paper outline

We develop an online optimisation method for RANS ML closure models to address challenges with overfitting to limited datasets, where the ML closure model is continuously updated during the simulation based on data from the evolving high-fidelity DNS flow field. This approach enables the closure model to adapt to the specific configuration being simulated, potentially overcoming the generalisation problem inherent to offline-trained ML closure models.

A fully online-trained RANS closure framework is developed that uses an embedded DNS subdomain to iteratively correct the closure model during the simulation itself. Importantly, this implies that the closure is trained directly on the geometry and physics of the target simulation, eliminating the mismatch between training and deployment. In this framework, high-fidelity regions within the RANS domain are simulated at DNS resolution, and their time-averaged quantities are used to compute a local loss. This loss is minimised via stochastic gradient descent to update the parameters of a neural network closure model embedded in the RANS solver. The result is a data-adaptive closure that evolves with the flow and corrects itself in situ.

The present study develops this methodology in a canonical setting, but the framework is general and extensible to other closures and nonlinear PDEs with unclosed terms. Our contributions include:

 The formulation of online-optimised RANS (oRANS), a coupled RANS/embedded DNS framework with continuous, online training of ML closures using data generated from the embedded DNS;
 The derivation of the discrete adjoint of the ML-augmented $k$--$$ turbulence model for PDE-constrained optimisation, together with an efficient numerical implementation; 
 The development of an inflow rescaling procedure that enables statistically representative embedded DNS without requiring long periodic boxes.

The paper is organised as follows. Section 2 introduces the oRANS algorithm in the setting of conservative PDEs, which provides the working formulation for the two systems studied here: stochastically forced Burgers’ equation and incompressible turbulent channel flow, and presents an efficient reverse-mode adjoint formulation. Section 3 validates the framework on the stochastic Burgers equation. Section 4 applies oRANS to the Navier--Stokes equations, presenting the governing equations and detailing numerical implementation, including an efficient autograd scheme leveraging the tridiagonal RANS discretisation, and deriving the adjoint for the ML-augmented $k$--$$ equations. Section 5 presents the numerical results of applying oRANS to turbulent channel flow across a range of $Re_$, where it consistently improves mean profiles and Reynolds stresses relative to a baseline and offline ML-RANS models, remains stable for modest embedded lengths where full periodic DNS spuriously laminarises, and scales linearly in cost with embedded length. Section 6 concludes with a summary and outlook.

The current formulation is limited by the representativeness of the embedded region, boundary-condition contamination, and the under-representation of long-wavelength modes in short domains. These limitations frame the scope of the present work and point toward future extensions, including multi-fidelity RANS/LES solvers with adaptive embedded subdomains.

Online-optimised RANS (oRANS) algorithm

The coupled RANS-eDNS system introduced above establishes the basic idea: a high-fidelity subdomain provides reference statistics, while the surrounding RANS domain supplies consistent boundary conditions. We now specialise this framework to the case of conservative PDEs. This class includes most physical systems of interest, and in particular directly covers the two systems studied here: the stochastically forced Burgers equation and incompressible turbulent channel flow. In this setting, the dynamics are expressed in terms of a state vector, fluxes, sources, and a closure operator, with additional algebraic constraints (e.g., continuity) appended where required. This conservative formulation is a concrete realisation of the generic RHS operator $$ introduced in section , and provides the working form for the adjoint optimisation strategy and algorithmic loop described below. 

Governing system and closure

Consider a general system of nonlinear conservation laws written in conservative form

 t 
 + (() - (, )) 
 = (), 
 x ,

where $(x,t)$ is the state vector, $$ the inviscid flux, $$ the viscous flux, and $$ source terms. Many systems also impose algebraic constraints, such as incompressibility $()=\!=0$, with pressure acting as a Lagrange multiplier. The approach presented below can be easily extended to such incompressible flows which include an additional continuity equation.

Upon averaging or coarse-graining (Reynolds or spatial averaging), unclosed terms appear. The low-fidelity formulationis then just the conservative analogue of the averaged system described in section ,

 t 
 + (() - (, )) 
 = () 
 + h(;),

where $h(;)$ is a closure operator parameterised by $$ (e.g.\ classical coefficients or neural-network weights). In practice, the low-fidelity system $$ is discretised in space and time, yielding a nonlinear residual system $(,)=0$. The high-fidelity equations are also discretised for simulation but are used solely to generate reference data and do not enter $$.

The simulation domain $$ is partitioned into a high-fidelity embedded subdomain $_$ and a low-fidelity subdomain $_$ with interface
\[
 = _ _.
\]
On $_$, the unclosed system~ is solved directly; on $_$, the closed system~ is solved. 
At the interface, the fields are coupled via a transfer operator

 |_ = 
 \!(|_;),

which supplies consistent boundary data to the high-fidelity embedded subdomain. 
Conversely, statistics of $$ may feed back into the low-fidelity closure parameters through $$, establishing a two-way coupling. 
In oRANS, $$ augments the low-fidelity mean field with rescaled fluctuations to provide statistically representative inflow. (Details for channel flow are given in section~.)

Deep neural parameterisation of the closure

In this work, the closure operator $h(;)$ is parametrised through a neural network $f_$. Concretely, the network maps local flow features $z$ (e.g. $, , ^2 $) to a set of effective closure parameters, which are then used to evaluate $h(;) = h(;f_(z))$. The architecture is designed to capture the strong nonlinear couplings and stiff source terms characteristic of turbulence 
closures. It consists of five hidden layers with two gated residual connections, defined recursively as

 H^1 &= (W^1 z + b^1), \\
 H^2 &= (W^2 H^1 + b^2), \\
 H^3 &= G^1 H^2, G^1 = (W^5 z + b^5), \\
 H^4 &= (W^3 H^3 + b^3), \\
 H^5 &= G^2 H^4, G^2 = (W^6 z + b^6), \\
 f_(z) &= W^4 H^5 + b^4,

where $$ denotes the Hadamard product, $$ is a hyperbolic tangent activation for physical smoothness and bounded output, the parameters $$ are the weights $W^k$ and biases $b^k$ of the neural network, and the gate layers $G^1$, $G^2$ are used to allow for modeling the strong nonlinearities expected of fluid turbulence models. We use a constant learning rate of \( = 10^-4 \) initially, followed by geometric decay to improve stability. Gradient updates are computed using RMSProp with zero momentum. We observe that model performance is not strongly sensitive to hyperparameter choices, provided sufficient averaging is maintained.

Objective functional and adjoint-based optimisation

The closure parameters $$ are optimised by minimising a mismatch between high and low-fidelity quantities over the embedded domain:

 J() 
 = _0^T __ 
 \!((), ) \, dx \, dt,

where $$ is a user-defined discrepancy. For the examples presented herein, we minimise the weighted square error in first- and second-order moments

 (,) = 2(|| - u^||_2^2 + w_k|| - k^||_2^2).

Note that all dependence of $J$ on $$ is through the state variables $()$.
After spatial and temporal discretisation, the low-fidelity system yields nonlinear residual equations $(,) = 0$, leading to the optimisation problem

 _ J(,)
 (,) = 0.

We form the discrete Lagrangian

 (,,) 
 = J() - ^ (,),

with adjoint variables $$. Here $_$ denotes the total derivative with respect to parameters $$, while $/()$ denotes partial derivatives holding other arguments fixed.
Differentiating $$ with respect to $$ along feasible trajectories (i.e., a solution $$ which satisfies $(,) = 0$) gives

 _ 
= ( - ^ ) d - 
^ .

Eliminating the computationally expensive Jacobian $d$ yields the discrete adjoint equations

 ( )^ = ( )^,

where $ / $ is the Jacobian of the nonlinear residual evaluated at the forward solution. Although the forward PDE solve is nonlinear, its adjoint is always linear, which enables the use of efficient linear solvers. Moreover, since the same Jacobian appears in the Newton iterations of the forward problem, the adjoint system can reuse the existing forward linear algebra infrastructure.

At feasible points where $=0$, $_ = _ J $. Therefore, the objective function gradient can be efficiently evaluated via

 _ J = - ^ .

Crucially, we do not differentiate through the high-fidelity solution; the high-fidelity fields enter $J$ as fixed reference data.
Rather than construct adjoint PDEs explicitly, we apply reverse-mode automatic differentiation to the scalar auxiliary function 

 (;) = ^ (,),

to construct the adjoint equation, treating $$ as fixed coefficients. Differentiation of the above scalar auxiliary function with respect to $$ reproduces the left-hand side of the discrete adjoint system~, while differentiation with respect to $$ yields the gradient of the objective function via equation~.

oRANS implementation

We discretise time with a fine grid $\t_n\_n0$ for PDE integration and a coarser sequence of parameter update times $\_m\_m0$ with $_m=t_n_m$ and $n_m+1-n_m=M$ (e.g.\ $M=100$). The parameters $$ are held fixed between updates:

 Initialise $$ on $_$, $__0$ on $_$, and set $_0$.
 For $m=0,1,2,$ until convergence:
 
 Forward PDE solve (fine loop): For $n=n_m,,n_m+1-1$, advance the coupled high/low-fidelity system~- from $t_n$ to $t_n+1$ with $=_m$ fixed, enforcing $|_=(|_;_m,t)$ at each step.
 Adjoint solve (coarse step): At $t=_m+1$, form and solve the nonlinear low-fidelity residual system $(__m)=0$
 to obtain the state $__m$. The adjoint variables are then computed by solving the linear system in equation .
 For steady low-fidelity systems (e.g.\ RANS or time-averaged Burgers), this adjoint is steady; for unsteady low-fidelity systems it is integrated backward over $[t_n_m,t_n_m+1]$.
 Parameter update:
 The adjoint solution provides the gradient of the objective with respect to the closure parameters through equation ,
 avoiding any need to compute $_ $. A gradient-descent step is hence applied:
 
 _m+1
 = _m + _m
 __m^_m+1
 __
 _ J(__m,) \, dx \, dt,
 
 
 with learning rate $_m$. Note that for quadratic choices of $$ including equation , the integrand reduces to the familiar form $(-)\,_ $.
 

 The key feature is its online nature: closure parameters are updated concurrently with PDE integration, in contrast to offline regression against precomputed datasets. This conservative-form specialisation of the generic framework in section underpins the specific implementations in section (Burgers) and section (channel flow).

Validation on Burgers equation

To verify the oRANS optimisation mechanics and evaluate its performance in a controlled setting, we first consider the stochastically forced, one-dimensional viscous Burgers equation. This canonical test case retains essential mathematical features of the Navier--Stokes turbulence cascade, including nonlinear advective and dissipative dynamics, while permitting detailed analysis and rapid numerical experimentation. 

Governing equations

We take the high-fidelity system as the stochastically forced, one-dimensional viscous Burgers equation, a specialisation of~. The state, fluxes, and source are

 = u(x,t), 
 &= 2u^2, 
 = \, x, \\
 &= f_(x,t) + _0 (x,t), 

where $$ is the Reynolds number, $f_$ is a deterministic forcing, and $(x,t)$ is a unit-variance stochastic process
. The coefficient $_0$ sets the
forcing amplitude.

Applying Reynolds decomposition $u = u + u'$ to the stochastically forced system yields the low-fidelity equation for the mean state

 t
 + u x
 = x^2
 + f_(x,t)
 + h( u;),

where the closure term $h( u;)$ represents the effect of the
unclosed correlation $2 $. 

Burgers equation closure modelling

The closure term can, in principle, be entirely represented by a neural network or a simple eddy-viscosity closure, for example the zero-equation toy model $_t=C_ _m() x$. Such a model can have large degrees of freedom but may not generalise well. Instead, we derive a single-equation turbulence model in the spirit of Boussinesq-type RANS closures and introduce an augmented low-fidelity state including the ``turbulent kinetic energy'' $k=2$, $ = \ u, k\$. 

In one dimension, assuming the Kolmogorov hypothesis, the Reynolds-stress term is modelled as

 = 2 C_ k^1/2_m x.

The turbulent kinetic energy then evolves according to

 t + u x
 = x\!(_t x)
 - Re C_D _m
 + 2 _t ( x)^2,

with $_t = C_ k^1/2_m$. In the conservative notation of section , the low-fidelity system can thus be written as

 = [ u, k]^T, &=[2 u^2, u k]^T, =[Re x - k,\ _t x]^T, \\
 &=[ f_, -ReC_D_m +2_t x x + k x]^T, 

and contains three turbulence parameters: $C_D$, $C_$, and $_m$. These are represented through the closure map

 C_D,\, C_,\, _m
 = f_(, , ^2 ;),

so that $$ parameterises the dependence of the closure coefficients on local mean-flow and $k$ features.

The turbulence model introduced above is not unique, and the oRANS framework allows for flexible balancing of physical modelling assumptions and machine learning closure. As in traditional offline machine learning approaches to turbulence modelling, the selection of a suitable model reflects a trade-off between the number of degrees of freedom and generalisability. However, since oRANS is trained on data from the in situ flow under identical boundary and physical conditions, generalisation constraints are relaxed compared to conventional machine learning approaches. This permits the use of more expressive machine learning models than would typically be feasible in offline settings.

Specification of the stochastic forcing

The stochastic forcing $$ is assumed uncorrelated in space and correlated in time, modelled as a sum of Ornstein-Uhlenbeck-driven Fourier modes,

 (x,t) \;=\;
_k\8,16,24,48\ X_k(t)(2 k x),

d X_k = -_k X_k\,d t + \,dW_k,

with independent Wiener processes $W_k( t )$, decay rates
$_k=\1,2,4,8\$, and noise amplitude $=10$. 
The prefactor $_0$ is then selected so that the space-time variance of the
forcing satisfies $=1$:

_0
\;=\;
(4_k_k^-1)^-1/2.

We additionally apply a deterministic, spatially periodic body-force

 f_(x) = _0(24 x),

which injects energy at a fixed wavenumber and maintains a statistically stationary mean flow.

Burgers equation results

Equations and are solved for $_0=0.146$, Re $=1300$ on a periodic domain $0 x 2$. The stochastic governing equation is solved on an embedded subdomain, while the remaining region is solved using the averaged Burgers equations. Derivatives are computed across the interface, which facilitates the exchange of momentum and energy fluctuations across the two models. The averaged model is run within the embedded region, using the same parameters, to compute gradient updates via the adjoint. Averages are taken over $M=1000$ independent realisations to provide target data for the closure model. A baseline case with no turbulence model $(=0)$ is included as a benchmark for comparison.

Figure shows results for two embedded domain sizes: (i) a full period of the longest forcing wavelength and (ii) a half-period, which cannot fully resolve the dominant mode. Here $L_e$ denotes the length of the embedded subdomain and $L_0$ the full periodic domain. In both cases, the online-trained model captures the true statistics more accurately than the baseline.
Quantitative comparisons are provided in table , where the relative errors are defined as

 &= _-)^2dx_ ( _ - _)^2dx, \\
 &= -k)^2 dx_ ( k_ - k_)^2 dx.

Here $_$ and $k_=2$ denote the time-averaged reference profiles obtained from solutions to the stochastic equation , $ u$ and $k$ are the corresponding predictions from the model under consideration, and $ u_$, $k_$ are the baseline predictions without a closure model.

The online-trained closure achieves a normalised $L_2$ error reduction of approximately 23\

These results indicate that the oRANS framework is robust to moderate under-resolution, and that it adapts effectively to local turbulence characteristics. This motivates future applications to Navier--Stokes flows and more complex geometries.

Navier--Stokes turbulence and closure modelling

Building on the Burgers verification case, we now move to apply oRANS to Navier--Stokes turbulence. We start by outlining the governing continuum equations alongside numerical implementation with results for the turbulent channel flow deferred to section .

DNS governing equations

The incompressible Navier--Stokes equations are solved in a three-dimensional cuboid subdomain \( _ ^3 \). In the notation of section~, the momentum equations are specified by the state, flux, and source vectors

 &= [u_i], \\
 () &= [u_i u_j + p_ij\,], \\
 (, ) &= 
 [ _b x_j\,], \\
 () &= [f_i\,], 

where \(u_i ^3\) is the velocity, \(p\) the pressure, 
\(_b = u_b (2)/\) the bulk Reynolds number based on bulk velocity \(u_b\), 
kinematic viscosity \(\), and full channel height \(2\), and \(f_i\) is an external forcing. Incompressibility is imposed as the algebraic constraint $
() u = 0$, with $p$ acting as a Lagrange multiplier.

Periodic boundary conditions are applied in the streamwise (\(x\)) and spanwise (\(z\)) directions, and no-slip wall conditions in the wall-normal (\(y\)) direction. In cases where fully developed periodic channel flow is not assumed, a Dirichlet inflow and convective outflow condition is imposed in the streamwise direction.

DNS numerical implementation

The governing DNS equations are discretised using a second-order central finite difference method on a staggered, structured grid. Velocity components are stored at cell faces, and pressure is located at cell centres. Temporal integration is performed using a classical four-stage Runge--Kutta (RK4) scheme applied to the momentum equations. A fractional-step projection method is used to enforce incompressibility, whereby the pressure is computed from a Poisson equation derived by taking the divergence of the momentum equation and applying the continuity constraint:

 ^2 p = - x_j x_i. 

The Poisson equation is solved at each Runge--Kutta substep using the BiCGStab iterative solver with a multigrid preconditioner from the HYPRE library. This step is GPU-accelerated and parallelised across MPI ranks, while the advection-diffusion updates are advanced using pure MPI halo exchanges. Homogeneous Neumann boundary conditions are applied to the pressure on all boundaries. In this formulation, pressure serves as a Lagrange multiplier that enforces the divergence-free constraint.

To maintain a prescribed bulk Reynolds number $Re_b$, a spatially uniform forcing term $f_i(t) = (f_x(t), 0, 0)$ is applied in the streamwise momentum equation. The magnitude of $f_x(t)$ is updated dynamically at each timestep to enforce

 V_ u(x,y,z,t)dV = 1,

where $V$ is the volume of the computational domain . The formulation is equivalent to imposing a time-dependent mean streamwise pressure gradient $-/ x=f_x(t)$, and the time-averaged forcing is equal to the mean pressure gradient required to sustain the prescribed bulk velocity (unity in nondimensional units).

The solver has been validated by reproducing the benchmark DNS results of at \( Re_ = 180 \), see , including mean velocity profiles, turbulence intensities, and Reynolds stress distributions. Grid convergence and timestep sensitivity was also verified at this Reynolds number. The simulation parameters for the cases considered herein are shown in table .

Time-averaged flow statistics are computed after the initial transients decay, typically after \( T_^+ = 500 \). Averaging is performed over an interval of \( T_^+ = 5000 \) viscous time units, which corresponds to approximately 50 eddy turnover times. Instantaneous fields are sampled at regular intervals for later analysis. The timestep is chosen to maintain a maximum CFL number below 0.5 in all simulations.

RANS governing equations

In the general $p=3$ case, the incompressible $k$--$$ transport equations for the mean velocity $$, turbulent kinetic energy $k$, and turbulent dissipation rate $$ are solved for $ = \ u_i, \ k, \ \$ over $_$,

 x_j( - 
 ) &= ,

where the inviscid-flux, viscous-flux, and source-term vectors are given by

 &= [\ u_j u_i + p_ij, \ u_j k, \ u_j ]^T, \\
 &= [_b x_j - , \ (_b + _k, _t) x_j, \ (_b + _,_t) x_j ]^T, \\
 &= [0, P_k - ^*_ k, \ _tP_k - _0, ^2 ]^T, \\
 P_k &= 2_t _ij \ _ij, \\
 _ij &= 2(_i x_j + _j x_i),

and where incompressibility is imposed as the algebraic constraint $
() u_i/ x_i = 0$.

The Boussinesq hypothesis closes the RANS equations through the turbulent viscosity $_t$,

 &= -2_t + 3k_ij \\
 _t &= _.

For channel flow with statistically homogeneous streamwise and transverse directions, the equations reduce considerably as the mean-flow quantities vary only in the wall-normal ($0 y L_2$) direction.

The unclosed RANS coefficients \( _k, _, ^*, _0, , \) are modelled as neural networks \( f_ \) with flow features as input variables, see section . These coefficients are optimised online to match high-fidelity statistics using an adjoint-based PDE-constrained optimisation framework.

The default $k-$ RANS constants are $_k = 1/2, \ _ = 1/2, \ ^* = 9/100, \ _0 = 3/40, \ = 5/9$, \ $=1$. Note that we include $_$ which allows the model to behave similarly to a $k--SST$ model for optimised neural network parameters $$. The boundary conditions at the walls are Dirichlet $(0) = (L_2) = 0$, $k(0) = k(L_2) = 10^-10$, $(0) = (L_2) = (3/40)d^2$, where $d$ is the distance to the nearest wall. 

RANS numerical implementation

The governing equations are advanced in a fully coupled manner, with the mean-flow and turbulence transport equations solved simultaneously. This monolithic treatment avoids splitting errors and naturally accounts for the coupling between velocity, turbulent kinetic energy, and dissipation rate.

The solution is updated according to a block semi-implicit scheme ,

 [ t + _x( - ) - ] &= -_x(^n - ^n_ ) + ^n.

The source term $$ is treated such that $k$ and $$ remain positive semi-definite by treating the production terms explicitly and dissipation terms implicitly. Specific to channel flow, where only $ u_1$ varies in $y$, the source vector reduces to

 = \
 
 0 &\\
 _t y y &- ^*kk^2 \\
 _t y yk &- _0^2
 
 \,

and $k$ and $_t y y$ are treated as constants. The model is evaluated with frozen gradients to avoid contaminating the Jacobian structure.

The mean pressure gradient in the RANS solver is dynamically adjusted to maintain a prescribed bulk velocity. The adjustment is implemented as

 x = - t (1 - u_b) - _w,

where \( _w = ._1 y|_ \) is the instantaneous wall shear stress. Here the channel half-width and bulk velocity have been normalised to unity.

To avoid forming dense Jacobians using automatic differentiation, we exploit the block-tridiagonal structure of the semi-implicit discretisation in equations --. Instead of differentiating the full residual, we apply reverse-mode AD to localised three-point stencils, i.e. the per-cell residual contributions, in parallel across the grid via batched Jacobian-vector products. This preserves sparsity and yields the three block diagonals directly, which we assemble into the Newton system and solve with a block-tridiagonal routine. In contrast to standard autograd that materialises a dense Jacobian, our stencil-wise approach reduces peak memory and wall time by more than an order of magnitude on the channel-flow cases reported while retaining the same linear algebra as the forward scheme. The resulting linear solves remain $O(N_2)$ in both time and memory for $N_2$ wall-normal cells, versus $O(N_2^2)$ memory if a dense Jacobian were constructed. This formulation enables efficient and stable implicit time stepping for ML-augmented RANS closures.

The RANS solver is validated against the Wilcox $k$-$$ model (default coefficients) in OpenFOAM for turbulent channel flow at \(Re_ = 180\), reproducing the benchmark data of .

Adjoint formulation

We define an objective functional that penalises mismatch in the first- and second-order moments,

J() = 2 __ _i=1^p\| ^_i, - _i^ \|_2^2 + \| k^_ - ^ \|_2^2 \, dx,

where \( _i^\) and \(^ \) are time-averaged quantities obtained from a concurrent DNS, and \( ^_i, \) and \( k^_ \) are computed from the current RANS model closure defined by \(\).

The governing RANS residuals are written compactly as $ R(,) = 0$, and the discrete Lagrangian then reads

 (,,) 
= J(,) - ^ R(,)

with adjoint variables $=( u_i, k,)$.
The objective-function gradient is obtained by solving the resulting adjoint equations. To derive these, consider the variation of the Lagrangian

 &= J + _ _i f_u^i dx + _ f_c dx + _ f_k dx+_ f_ dx \\& +_(_i f_u^i+f_c+f_k+f_)n_k x_k dx,

where for the $k$--$$ system, the residual vector takes the form $ = [f_c, f_u^i, f_k, f_]^T$, where $f_c$ is the continuity residual, $f_u^i$ the momentum residuals, $f_k$ the turbulent kinetic energy residual, and $f_$ the specific dissipation residual. $$, $$ define the inner and boundary regions respectively. The first-order variations are then given by

 f_c &= x_i, \\
 f_u^i &= x_j + x_j + x_i \\&- x_j[ (_b+_t )( x_j + x_i ) ] - x_j[ _t( x_j + x_i)], \\
 f_k &= x_j
 + x_j - x_j[(_b + _k, _t) x_j ] \\& - x_j(_k, _t x_j) - P_k + ^*_ k + ^*_ k , \\
 f_ &= x_j
 + x_j - x_j[(_b + _, _t) x_j ] \\&- x_j(_, _t x_j) - _ k P_k - _ k P_k + _ k^2P_k k + 2_0, , \\
 _t &= _ ( k - ^2 ).

Integrating by parts and collecting like terms for the inner domain $$ then yields the adjoint equations

_i x_i &=0, \\
- _i x_j + x_i_j - x_j
[(_b+_t)
(_i+ x_i)] \\ + x_i + x_i + x_i + 2 x_j(_t\,S_ij\, + _\,S_ij 
) &= , \\
- x_j -\; x_j[
(_b+_k,_t)
 x_j] - (k-^*_)\, \\+ 
( x_j+ x_i)
_i x_j +\,
 x_j\,
 x_j + x_j\,
 x_j &= k, \\
- x_j -\; x_j[
(_b+_,_t)
 x_j] + 2_0, + ^*_ k \\- ^2
( x_j+ x_i)
_i x_j - k^2 x_j x_j - k^2 x_j x_j &= .

Stationarity in $$ therefore gives the discrete adjoint equations, while differentiation in $$ yields the gradient needed for optimization:

_ J = -^ .

In practice, however, we do not explicitly discretise the continuous adjoint PDEs expressed above. As described in section~, we instead evaluate $^ R(,)/$ and $^ R(,)/$, resulting from differentiating with respect to $$, via reverse-mode automatic differentiation applied to a scalar auxiliary function $ = ^ R(,)$, treating $^$ as constant over single adjoint time steps. This allows us to reuse the forward solver infrastructure for the adjoint system and obtain the gradient without forming Jacobians explicitly.

To verify the adjoint implementation, we compare adjoint-computed gradients with finite-difference approximations obtained by perturbing the viscosity $1/_b$. The finite-difference gradient is computed as $ J^ = (J(1/Re_b+) - J(1/Re_b-)) / (2)$, where $$ is the perturbation size.
Figure~ shows the resulting relative gradient error as a function of the finite-difference step size~$$. The expected V-shaped curve, with a low minimum error, confirms the correctness of the adjoint formulation.

Deep learning closure model

We now consider applications to the statistically 1D turbulent channel flow case.
We initially consider three different strategies for determining the closure coefficients $_p = \, \ ^*,_0,_k,_,\$ or their neural‐network generalisations. In all cases, we denote by $$ the parameters to be optimised.

First, consider a parametric model, where the six $k$--$$ closure constants are treated as global scalars i.e.

 _p = _p = (, \ ^*,\,_0,\,_k,\,_,\,),

where $_p ^6$ are directly optimised to minimise the mismatch with DNS data.

Second, consider a global feature model, where the closure parameters vary with wall distance, expressed as a function of the non-dimensional coordinate $y^+ = y_ u_/$, where $y_$ is the nearest distance to the wall:

 _g(y) = f_(y^+),

where $f_$ is a fully connected network with parameters $$.

Finally, consider a local flow feature model, where the closure coefficients depend on local dimensionless flow invariants,

 _l(y) = f_(S^*, Re_T,( y)^+, ( y)^+).

 The shear rate $S^* = y$ represents the shear to dissipation balance; the turbulent Reynolds number $Re_T = $ the turbulence intensity; $( y)^+ = y k^1.5$ represents dimensionless turbulent kinetic energy transport and $( y)^+ = y^2$ represents the dimensionless turbulent dissipation transport. The network inputs are constructed from dimensionless local invariants, ensuring that the model is scale- and rotation-invariant and generalisable across different Reynolds numbers.
As will be shown in section~, the local flow feature model $_l$ provides sufficient flexibility to accurately reproduce DNS statistics across a range of Reynolds numbers. Accordingly, we adopt this model exclusively during the online closure optimisation phase.

The inputs to the network are normalised to remain order $O(1)$. 
Each input feature

 = [ S^*, \, Re_T, \, ^+, \, ( y)^+, \, ( y)^+]^,

is hence divided by a corresponding normalisation coefficient $_ = [ 4, \, 10, \, 1.5 10^5, \, 25, \, 10 ]^$ to ensure consistent magnitudes across different quantities,

_ = _^-1.

Offline validation of ML closure capacity

We first present supervised fits of the DL closure models directly to DNS data for turbulent channel at $Re_=180$, as a demonstration of model flexibility. The training targets are generated a priori from long DNS time averages. The results are shown in figure .

All models, including the default $k-$ model, match the mean velocity profile well. The turbulent kinetic energy is however poorly predicted by the default model, with peak TKE production near half of the DNS prediction, and estimated closer to the centre of the channel.

The parametric model improves on the default model for turbulence statistics predictions, but a more complex model is clearly required to match the DNS data, demonstrating the fundamental limitation of the $k-$ formulation. The NN models are shown to be flexible enough to achieve this, reproducing both velocity and TKE statistics, including the near-wall peak of $k^+$, and convergence is near monotonic and rapid, with little quantifiable difference between local and global models.

The local flow feature neural network (see equation ) RANS model is clearly expressive enough to accurately model channel flow turbulence, and relies only on invariant local inputs. Additionally, convergence is near monotonic and rapid, with little quantifiable difference between local and global models. We hence adopt the local model architecture for the subsequent online training and deployment within the RANS-embedded DNS framework, while noting that the present results serve only to demonstrate representational capacity rather than generalisation.

oRANS setup for turbulent channel flow

Having established the representational capacity of the neural closure, we next describe its deployment within the oRANS framework for turbulent channel flow. The computational domain $$ is partitioned into a RANS subdomain $_$ and an embedded DNS region $_$, such that $ = _ _$. The two solvers exchange boundary conditions across the common interface $ = _ _$.

To benchmark this approach, we consider a target flow at Reynolds number \( Re_2 \) and inject inflow fluctuations derived from a separate fully developed DNS at \( Re_1 \). These are rescaled according to

	u^_ = _^ 
 + ^k^_Re_1\,
 u^' _Re_1 := , 
 x ,

and used to drive the embedded DNS at \( Re_2 \), while a closure model is concurrently optimised online to reduce the mismatch between DNS and RANS statistics. Although the DNS fields are unsteady, their statistics converge as $t $. The oRANS closure therefore adapts to reduce statistical error rather than instantaneous mismatch, enabling generalisation across Reynolds numbers. The solution in $_$ depends on $$ through its interface coupling $$ with $_$, and conversely the RANS closure adapts through statistical feedback from $_$.

In practice, the transformation $$ need not be limited to the simple rescaling used here. More sophisticated formulations may be required for complex flows, and a natural extension would be to replace $$ with a turbulence-inflow generator consistent with both RANS statistics and local flow features. Moreover, the objective can be augmented to train on additional DNS statistics beyond $( u,k)$; for example, including $$ enables timescale learning and allows $$ to be conditioned on $( u, k, )$. By bringing the inlet fluctuations closer to a Navier-Stokes-consistent state, such extensions are expected to reduce boundary-condition pollution and shorten transient adjustment phases. We do not pursue these variants here, but they represent natural directions for extending the framework.

The overall computational workflow is shown in figure . The RANS-ML solver (Node 0) computes mean fields $^_$ and adjoint sensitivities. These statistics rescale stored inflow fluctuations from $Re_1$ DNS, providing inlet conditions to the embedded DNS at $Re_2$ via $$. The embedded DNS is run in parallel across multiple nodes, producing high-fidelity fields $^$ that are accumulated in a replay buffer. Time-averaged statistics from this buffer define the mismatch with the RANS solution, yielding sensitivities that drive the update of $$. The full training procedure is detailed in algorithm .

Time grids: define the fine grid \( \t_n\_n0 \) with \( t_n = n \).
Define coarse update times \( \_m\_m0 \) by \( _m = t_n_m \) with \( n_m+1-n_m = M \).

 Generate turbulent fluctuations from a periodic DNS at \( Re_1 \)
 Initialise a replay buffer \(\) using one flowthrough of eDNS data at \(Re_2\) seeded with the \(Re_1\) fluctuations.
 
 Pretrain closure model parameters \( _0 \) on baseline RANS \( k- \) fields
 For each parameter update time \(_m\), with \(m=0,1,2,\):
 
 DNS stepping (fine loop): For \(n=n_m,,n_m+1-1\), run eDNS at \(Re_2\) with inflow
 \[
 u'_(t_n)
 = k^\,
 u'^(t_n) + ^ ,
 \]
 and append downstream statistics \(u^(t_n)\) to the replay buffer \( = [u^(t_n_m-K),\;,\;u^(t_n_m)]\)
 Remove old samples from \(\) with times \(t<t_n_m-K\).
 Randomly sample mini-batches \(v \) and compute
 time-averaged statistics \(_, _\).
 Parameter update:
 
 _m+1 = _m + _m
 __\!
 (v - ^__m)\,
 _ ^__m\, dx.
 
 Apply RMSProp with learning rate \(_m\), asymptotically minimising

J()
= __
(
\|
_TT_0^T u^_(t,x)\,dt
- ^_(x)
\|_2^2
\;\\+ \ 
w_k(
_TT_0^T k^_(t,x)\,dt
- k^_(x)
)^2
)\,dx,

where $^_$, $k^_$ are solved on $_$.
 Recompute the RANS solution with updated \(_m+1\).
 

Turbulent channel flow numerical results

With solvers and closures defined, we now present numerical experiments on turbulent channel flow. We begin with a summary of the main findings, before detailing the results and analysis in section onward. Across all tested Reynolds numbers, oRANS achieves consistently lower errors than both the baseline $k$-$$ and offline DL closures. Crucially, it maintains accuracy with modest embedded regions ($L_x 2$), whereas DNS in shortened periodic boxes spuriously laminarises and produces qualitatively incorrect profiles. Because only the embedded region is resolved at high fidelity, oRANS also delivers a clear computational advantage: the cost scales approximately linearly with embedded length, enabling accurate training at a lower cost as compared to full-domain DNS/LES. At the same time, the results highlight key limitations that will guide future development. Performance degrades when boundary-condition pollution contaminates the interior of short subdomains or when low-wavenumber modes are under-represented.

Shortened streamwise domain periodic DNS

As a preliminary diagnostic, we examine the behaviour of DNS in shortened periodic domains.

This analysis provides guidance for the oRANS framework: the minimum length of the embedded DNS subdomain $_$ must be large enough to sustain realistic turbulence. When the streamwise extent of a periodic DNS box is reduced below this threshold, the turbulence dynamics become distorted and in some cases the flow re-laminarises altogether. Figure shows velocity and TKE profiles at $Re_=180$, where laminarisation is observed for very short boxes (e.g. $L_x=3$). At higher Reynolds numbers in the present study, laminarisation was not observed; however, the outcome may depend on the initial condition and the basin of attraction of the turbulent state, an issue we regard as beyond the present scope. 

To quantify the departure from reference DNS statistics, we introduce a normalised cost functional. For each quantity $q \ u,k\$ we define

J_q = 2_0^ (q^oRANS(y) - q^(y))^2 dy2_0^ (q^(y) - q^(y))^2 dy,

and combine them as

J^* = 1+w_k(J_u0 + w_k J_k0),

where $w_k=5$ and $J_u0, J_k0$ are the baseline $k$--$$ RANS errors relative to DNS. By construction, $J^* = 1$ corresponds to the unmodified $k$--$$ model, values below $1$ indicate improvement, and very large values correspond to laminarisation.

Table shows that the predicted flow statistics deteriorate significantly when the DNS domain length is reduced below approximately \( L_x,0/3 \), with very large values corresponding to laminarisation. For domains that remain turbulent, the distortion of statistics manifests primarily as an overprediction of turbulent kinetic energy. This is evident at $L_x=3\,$ in figure , where the mean velocity is well captured but the TKE exhibits a clear overshoot, a representative pathology of shortened-domain simulations more generally.

This behaviour is consistent with the concept of a minimal flow unit: for \( Re_ 180 \), sustaining near-wall turbulence requires a streamwise extent of at least \( L_x^+ 300 \) , which corresponds to approximately \( L_x,0/8 \) in physical space. Below this threshold, the turbulence regeneration cycle is suppressed and the flow reverts to a laminar state. At higher Reynolds numbers the required physical length increases, reflecting the growth of outer-layer structures even as near-wall structures remain of fixed size in wall units.

oRANS channel flow experiments

We now present the results following the oRANS approach detailed in algorithm . We summarise the numerical studies considered herein in table . To place oRANS in context, we compare against three references of increasing fidelity: (i) fully periodic channel-flow DNS with shortened streamwise domains, providing a high-fidelity reduced-cost reference, (ii) a rescaled offline-trained $k$--$$ RANS without online adaptation, representing the standard offline ML strategy, and (iii) state-of-the-art turbulence inflow generation methods , which provide synthetic turbulence fluctuations at the inlet based on target statistics.

We simulate a full periodic DNS at a reference Reynolds to generate turbulent statistics. We then re-scale the statistics based on the online trained RANS solution to the target Reynolds number across a range of channel lengths $L_x,0/8 L_x L_x,0$. Of particular interest is how the statistics degrade for shorter channel lengths, and how rapidly the statistics converge downstream of the inlet.

Table~ summarises the training results across computed Reynolds numbers and channel lengths, expressed in terms of the normalised cost functional (equation ). The results show that the baseline $k$--$$ RANS model exhibits large errors, particularly at higher $Re_$. In all cases, this is largely due to a misprediction in the location and magnitude of peak turbulent kinetic energy in the channel. Offline DL training on the full periodic domain significantly improves predictions compared to baseline RANS, but the generalisation to out-of-sample Reynolds numbers is limited. In contrast, the online-optimised RANS approach achieves consistently lower errors across all Reynolds numbers, even when trained with relatively small DNS subdomains. Accurate training requires only modest inflow lengths ($L_x 2$), but sensitivity to domain size becomes more pronounced at higher $Re_$. For very short subdomains ($L_x 3$), oRANS performance significantly degrades, and in some far out-of-sample cases training diverges entirely (indicated by ``--'' in the table), primarily due to boundary condition contamination.

To better illustrate the error mechanisms underlying these summary metrics, we present example velocity and turbulent kinetic energy profiles of cases II, IV for $L_x/L_x,0 = 6$ in figure comparing the differences in RANS, traditional offline supervised physics informed ML workflows and oRANS. All other cases show comparable profile distributions. 

For the channel flow cases considered, all three models reproduce the mean velocity profile with good accuracy. However, this agreement is not expected to generalise to more complex flows, where RANS closures are known to mispredict the mean profile . The turbulent kinetic energy distribution is however poorly predicted by the default RANS model, with significant underproduction in the buffer to log-layer transition and mild overproduction in outer layer. For the mildly out-of-sample case II, offline supervised RANS-ML accurately corrects this error, and oRANS performance is comparable. In the far out-of-sample case IV however, the offline supervised workflow begins to fail but still outperforms the RANS. oRANS comparatively successfully recovers the distribution, particularly in the peak production region where offline supervised RANS-ML begins to deviate from the DNS.

More broadly, out-of-sample degradation in offline training is expected to be even more pronounced for turbulence models that rely more heavily on machine learning; for instance, when the entire Reynolds stress tensor is represented as a neural network output. In such cases, the lack of physical anchoring increases sensitivity to distributional shift, and the importance of embedded physics-informed strategies are expected to be more pronounced.

We now turn from comparing models to examining the role of domain size in oRANS performance. Consider the effect of shorter streamwise domains on oRANS, shown in figure for weakly out-of-sample ($Re_=180$) and far out-of-sample ($Re_=590$) cases.

For the weakly out-of-sample case, oRANS performs well even with very short domains, although the turbulent kinetic energy is mildly underpredicted in the outer layer. In contrast, for the far out-of-sample case, the limitations of short domains become evident: training diverges as domain length decreases. This deterioration is also seen in the mean velocity profiles, which fall below even baseline RANS predictions. The failure arises due to a self-reinforcing instability associated with boundary condition pollution. For far out-of-sample cases, both the inlet and outlet boundaries are less likely to closely respect the Navier--Stokes dynamics. Because the governing equations are elliptic in nature, boundary errors propagate throughout the domain, with the strongest impact in the vicinity of the boundaries. These polluted statistics are then recycled into the training, amplifying the error and ultimately driving divergence. These results indicate that while modest embedded domains suffice for training, excessively short domains cannot be used reliably for out-of-sample predictions. More generally, the appropriate domain size is problem-specific, and accurate boundary-condition representation is crucial for robust oRANS performance.

We present the convergence history of oRANS and periodic DNS for weakly and significantly out-of-sample cases in figure . Here $t^*$ denotes the normalised simulation time, defined by scaling the wall-clock time by the characteristic convergence window of the reference DNS. At $Re_=180$, the baseline DNS exhibits a prolonged transient in which the weighted error initially grows before eventual decay, whereas oRANS rapidly settles: the rescaled fluctuations yield near-monotonic convergence across the embedded domain lengths. For the more challenging $Re_=395$ case, the same qualitative behaviour is observed, with oRANS showing near monotonic convergence, but the wall-clock time to reach a given tolerance is comparable to the periodic DNS. This reflects the longer streamwise development and feedback delay at higher $Re_$ together with a larger mismatch between the inlet rescaling and the true statistics. For consistency, the periodic DNS fields are initialised by rescaling the fluctuations of the baseline DNS using the default $k$--$$ RANS. As shown in figure , for $Re_=395$ the periodic DNS converges substantially slower if restarted directly from the baseline DNS field without rescaling. The computational burden is dominated by the DNS component, as the cost of the one-dimensional RANS model and its adjoint is negligible and can be evaluated in parallel. Consequently, the per-timestep cost of oRANS scales approximately linearly with the embedded domain fraction, $L_x/L_x,0$. Shorter domains further reduce the effective feedback time, since the data-collection plane lies closer to the inlet and the characteristic flowthrough time is smaller. 

However, if the collection plane is placed too close to the inlet, the statistics are contaminated by transient adjustment effects, which introduce noise into the training and can misdirect the gradient. This tradeoff highlights both the efficiency and the limitations of short-domain training in oRANS.
Finally, we note that oRANS introduces additional I/O overhead compared to periodic DNS. In the present implementation, the reference DNS dataset must be retained in memory, and the replay buffer must be saved on write. The buffer additionally introduces additional communication overhead, which may become significant at scale, though it is minor compared to the DNS cost in the present setup.

Turbulence representation

To better understand how oRANS adapts to different flow regimes, we briefly analyse detailed turbulent statistics for example training flows.

First consider the learned turbulence model coefficients across Reynolds numbers. Figure~ shows the wall-normal variation of the key closure terms $$, $_0$, $^*$, $$, $_k$, and $_$ obtained from the online-trained models. The dashed lines denote the constant baseline values used in the standard $k$--$$ model.

A key distinction emerges between offline-trained models and oRANS. Offline training produces a single, fixed set of parameters that cannot adapt to new Reynolds numbers. In contrast, the online optimisation modifies the coefficients in a Reynolds-number-dependent way, reflecting changes in turbulence production and transport. These adjustments are not imposed a priori but arise naturally from the coupled training with DNS statistics, suggesting that the learned corrections adapt to changes in large-scale turbulence dynamics.

These variations highlight two important aspects of the oRANS approach. On the one hand, the learned parameters remain close to the standard constants in the viscous sublayer, indicating that the model respects near-wall asymptotics without needing explicit enforcement, but acquires non-trivial wall-normal and Reynolds-number dependence elsewhere. By breaking the rigidity of constant-coefficient closures, oRANS dynamically reshapes the turbulence representation in response to the flow, something an offline-trained ML closure cannot achieve.

Next, figure compares the streamwise spectra and two-point velocity correlations against the fully periodic case, for example, case II.
The channel flow spectra highlight an important limitation of the embedded DNS strategy: whenever the high-fidelity sub-domain is too short to contain the largest energetic structures, the low-wavenumber content of the spectrum cannot be represented, and the optimisation of the $k$-equation inevitably degrades. In a truncated box the very low streamwise wavenumbers, which originate in the channel centre where the largest eddies reside, cannot be represented, and this is precisely where the relative error in the optimised $k$ profile becomes visible. The short-box tests therefore constitute an especially stringent, ``worst-case'' scenario for oRANS; that the method still yields reasonable agreement is encouraging.

At the same time, the high-wavenumber range of the spectra, the cascade, and the two-point correlations remain accurately reproduced even for modest box lengths. This indicates that oRANS faithfully captures small-scale turbulent dynamics, while its limitations are confined to the very largest structures excluded by the truncated domain.

Finally, we examine the streamwise evolution of turbulence statistics in short embedded domains. Figure~ shows the example case at $Re_=180$.
Across the Reynolds-numbers tested, we find that the re-scaled fluctuations recover statistically stationary turbulence within one integral length downstream of the inlet: in cases where oRANS matches DNS (see table ) the mean profiles of $u$, $k$, and $$ are already indistinguishable from fully developed DNS data by $x = 1.25$. This is markedly faster than state-of-the-art synthetic-inflow techniques such as the RANS-guided method of , which reach comparable accuracy only after $x6$. The key difference is that oRANS feeds the DNS sub-domain with physically consistent, dynamically evolving fluctuations from an existing DNS dataset rather than statistically prescribed surrogates which requires a longer relaxation distance, but have the advantage of only requiring a baseline RANS solution. The limitation of the approach is equally clear: if the embedded box is too short to represent the lowest streamwise wavenumbers, the large-scale energy is systematically absent. In this case, higher-order moments converge quickly, but their equilibrium differs from the true mean depending on the domain length. This represents a hard constraint on accuracy that cannot be overcome by training alone.

From an application perspective, this restriction may be mild: in many engineering configurations, the largest eddies are of the order of a characteristic geometry scale, while the computational domains span many such scales. For example, in a wind-farm setting, an LES patch of one or two rotor diameters embedded in a RANS domain covering dozens of turbines would resolve the relevant large-scale structures. In such cases, as our longer-box tests confirm, oRANS recovers global statistics with high fidelity. Nevertheless, careful consideration of the largest turbulent scales relative to the chosen embedded patch remains essential when extending oRANS beyond canonical channel flow.

Conclusion

We have introduced oRANS, an online optimisation framework that couples a RANS solver with an embedded DNS/LES sub-domain. By training directly on in situ data from the target flow, oRANS circumvents the data-sparsity and over-fitting issues that limit offline machine-learning closures and is, in principle, applicable to any nonlinear PDE with unresolved terms. Validation on both the stochastically forced Burgers equation and turbulent channel flow shows that oRANS consistently outperforms offline ML training, particularly in far out-of-sample test cases. It maintains accuracy even for modest embedded domains where full periodic DNS may spuriously laminarise, and achieves modest computational savings compared to full high-fidelity simulation, with cost scaling approximately linearly with the embedded domain size.

Beyond demonstrating proof of concept, the work contributes several methodological advances. We derived and implemented the discrete adjoint of a DL-augmented $k$--$$ model for PDE-constrained optimisation, enabling efficient gradient computation within the RANS framework. A semi-implicit block-segregated RANS solver was developed with a stencil-wise reverse-mode implementation that preserves sparsity, yielding fast and stable time stepping for ML-augmented closures. We also introduced a rescaled inflow procedure that allows statistically representative embedded DNS without requiring long periodic boxes, thereby accelerating convergence and improving robustness compared to synthetic inflow techniques. Together, these advances establish a general framework for coupling low- and high-fidelity solvers in a way that supports online training and scalable deployment.

At the same time, we highlighted important limitations. The chief limitation is boundary-condition pollution: when the embedded domain is too short, spurious boundary effects contaminate the interior statistics, leading to self-reinforcing errors and, in extreme cases, algorithmic divergence. Exclusion of the lowest wavenumber modes for short domains also reduces accuracy, since the largest turbulent structures cannot be represented. These challenges are particularly pronounced in turbulent channel flow, where simulations are often configured so that the largest turbulent structures are comparable to the domain size. In many applied flows, by contrast, the characteristic turbulent scales are much smaller than the overall domain size.

When one (or several) embedded subdomains span the dominant physics, as is typical in quasi-homogeneous flows over many integral length scales, oRANS recovers full-domain low order statistics with high fidelity while retaining computational efficiency. The same optimisation strategy naturally extends to multi-block layouts and complex geometries, with accuracy expected to taper only as the flow becomes strongly heterogeneous. This establishes online optimisation with embedded data generation as a scalable route to data-adaptive closures, bridging high- and low-fidelity solvers across a broad class of nonlinear PDEs, from turbulence modelling to other multiscale systems.

DNS Validation

To validate the DNS solver, we reproduce the canonical turbulent channel flow dataset of at $Re_ 180$. This case is a standard benchmark in the turbulence literature and provides both integral statistics and detailed turbulence profiles against which new solvers can be checked. Our simulation achieves a friction Reynolds number of $Re_ = 175$, close to the reference value.

Figure~ compares mean velocity, turbulent kinetic energy, and Reynolds stress components between our DNS and the KMM dataset. Overall agreement is excellent across the channel, with small differences attributable to the slightly lower $Re_$ of our simulation. The near-wall peak in the streamwise stress and the location of the Reynolds shear-stress maximum are well captured. Minor discrepancies appear for the wall-normal stress $$ at the channel centre which is slightly underpredicted, and the peak spanwise stress $$ which is slightly overpredicted as compared to .

Table~ reports integral flow statistics. Bulk Reynolds number, velocity ratios, and the skin-friction coefficient all lie close to the reference values, confirming that the present solver reproduces canonical channel flow at this Reynolds number with high fidelity.

Together, these results establish that the present DNS implementation is consistent with benchmark data and provides a reliable high-fidelity reference for the oRANS framework.